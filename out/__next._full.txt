1:"$Sreact.fragment"
2:I[39756,["./_next/static/chunks/2f236954d6a65e12.js"],"default"]
3:I[37457,["./_next/static/chunks/2f236954d6a65e12.js"],"default"]
4:I[63780,["./_next/static/chunks/f813fde7a26bc4e5.js"],"Navbar"]
11:I[68027,["./_next/static/chunks/2f236954d6a65e12.js"],"default"]
:HL["./_next/static/chunks/4802f24a37f5a205.css","style"]
:HL["./_next/static/media/70bc3e132a0a741e-s.p.15008bfb.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["./_next/static/media/83afe278b6a6bb3c-s.p.3a6ba036.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"ENut39GRpSJmqbt7mRWeS","c":["",""],"q":"","i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"./_next/static/chunks/4802f24a37f5a205.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"inter_8f1c4a24-module__nx7jFG__variable jetbrains_mono_4282af4f-module__TNdyLG__variable font-sans antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[[["$","$L4",null,{}],["$","main",null,{"children":[["$","section",null,{"className":"relative flex min-h-screen items-center justify-center px-6 pt-20","children":[["$","div",null,{"className":"mx-auto max-w-3xl text-center","children":[["$","p",null,{"className":"mb-4 font-mono text-sm tracking-widest text-primary uppercase","children":"Hello, I am"}],["$","h1",null,{"className":"text-balance text-4xl font-bold leading-tight tracking-tight text-foreground sm:text-5xl lg:text-6xl","children":"Sarvesh Angadi"}],["$","h2",null,{"className":"mt-4 text-xl font-medium text-foreground/80 sm:text-2xl","children":"Robotics & Computer Vision Engineer"}],["$","p",null,{"className":"mt-3 font-mono text-sm tracking-wide text-muted-foreground","children":"Perception · Learning Systems · Teaching"}],["$","div",null,{"className":"mt-8 flex items-center justify-center gap-4","children":[["$","a",null,{"href":"https://github.com","target":"_blank","rel":"noopener noreferrer","className":"inline-flex items-center gap-2 rounded-lg bg-foreground px-5 py-2.5 text-sm font-medium text-background transition-opacity hover:opacity-90","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github h-4 w-4","aria-hidden":"true","children":[["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}],["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}],"$undefined"]}],"GitHub"]}],["$","a",null,{"href":"https://linkedin.com","target":"_blank","rel":"noopener noreferrer","className":"inline-flex items-center gap-2 rounded-lg border border-border px-5 py-2.5 text-sm font-medium text-foreground transition-colors hover:bg-muted","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-linkedin h-4 w-4","aria-hidden":"true","children":[["$","path","c2jq9f",{"d":"M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"}],"$L5","$L6","$undefined"]}],"LinkedIn"]}]]}]]}],"$L7"]}],"$L8","$L9","$La","$Lb","$Lc"]}],"$Ld"],["$Le"],"$Lf"]}],{},null,false,false]},null,false,false],"$L10",false]],"m":"$undefined","G":["$11",[]],"S":true}
23:I[97367,["./_next/static/chunks/2f236954d6a65e12.js"],"OutletBoundary"]
24:"$Sreact.suspense"
26:I[97367,["./_next/static/chunks/2f236954d6a65e12.js"],"ViewportBoundary"]
28:I[97367,["./_next/static/chunks/2f236954d6a65e12.js"],"MetadataBoundary"]
5:["$","rect","mk3on5",{"width":"4","height":"12","x":"2","y":"9"}]
6:["$","circle","bt5ra8",{"cx":"4","cy":"4","r":"2"}]
7:["$","a",null,{"href":"#about","className":"absolute bottom-10 left-1/2 -translate-x-1/2 animate-bounce text-muted-foreground","aria-label":"Scroll to about section","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-down h-5 w-5","aria-hidden":"true","children":[["$","path","s699le",{"d":"M12 5v14"}],["$","path","1idqje",{"d":"m19 12-7 7-7-7"}],"$undefined"]}]}]
8:["$","section",null,{"id":"about","className":"px-6 py-24","children":["$","div",null,{"className":"mx-auto max-w-5xl","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"h-px w-8 bg-primary"}],["$","span",null,{"className":"font-mono text-sm tracking-widest text-primary uppercase","children":"About"}]]}],["$","h2",null,{"className":"mt-2 text-3xl font-bold tracking-tight text-foreground sm:text-4xl","children":"Building intelligent machines that see and understand"}],["$","p",null,{"className":"mt-6 max-w-2xl text-base leading-relaxed text-muted-foreground","children":"I am a robotics engineer with a focus on computer vision and perception. My work sits at the intersection of classical geometry-based methods and modern deep learning, enabling autonomous systems to interpret complex visual scenes. I have contributed to research in camera calibration, visual SLAM, and learning-based perception deployed on real-world robotic platforms."}],["$","div",null,{"className":"mt-16 grid gap-8 sm:grid-cols-2","children":[["$","div","Perception Systems",{"className":"group rounded-xl border border-border bg-card p-6 transition-colors hover:border-primary/30","children":[["$","div",null,{"className":"mb-4 inline-flex h-10 w-10 items-center justify-center rounded-lg bg-primary/10 text-primary","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-eye h-5 w-5","aria-hidden":"true","children":[["$","path","1nclc0",{"d":"M2.062 12.348a1 1 0 0 1 0-.696 10.75 10.75 0 0 1 19.876 0 1 1 0 0 1 0 .696 10.75 10.75 0 0 1-19.876 0"}],["$","circle","1v7zrd",{"cx":"12","cy":"12","r":"3"}],"$undefined"]}]}],["$","h3",null,{"className":"text-base font-semibold text-card-foreground","children":"Perception Systems"}],["$","p",null,{"className":"mt-2 text-sm leading-relaxed text-muted-foreground","children":"Building real-time 3D perception pipelines for autonomous robots using LiDAR, stereo vision, and sensor fusion."}]]}],["$","div","Camera Calibration",{"className":"group rounded-xl border border-border bg-card p-6 transition-colors hover:border-primary/30","children":[["$","div",null,{"className":"mb-4 inline-flex h-10 w-10 items-center justify-center rounded-lg bg-primary/10 text-primary","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-camera h-5 w-5","aria-hidden":"true","children":[["$","path","18u6gg",{"d":"M13.997 4a2 2 0 0 1 1.76 1.05l.486.9A2 2 0 0 0 18.003 7H20a2 2 0 0 1 2 2v9a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V9a2 2 0 0 1 2-2h1.997a2 2 0 0 0 1.759-1.048l.489-.904A2 2 0 0 1 10.004 4z"}],["$","circle","1vg3eu",{"cx":"12","cy":"13","r":"3"}],"$undefined"]}]}],["$","h3",null,{"className":"text-base font-semibold text-card-foreground","children":"Camera Calibration"}],["$","p",null,{"className":"mt-2 text-sm leading-relaxed text-muted-foreground","children":"Developing robust multi-camera calibration frameworks for accurate metric reconstruction in industrial environments."}]]}],["$","div","Deep Learning",{"className":"group rounded-xl border border-border bg-card p-6 transition-colors hover:border-primary/30","children":[["$","div",null,{"className":"mb-4 inline-flex h-10 w-10 items-center justify-center rounded-lg bg-primary/10 text-primary","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-brain h-5 w-5","aria-hidden":"true","children":[["$","path","adv99a",{"d":"M12 18V5"}],["$","path","1e3is1",{"d":"M15 13a4.17 4.17 0 0 1-3-4 4.17 4.17 0 0 1-3 4"}],"$L12","$L13","$L14","$L15","$L16","$L17","$undefined"]}]}],"$L18","$L19"]}],"$L1a"]}]]}]}]
9:["$","section",null,{"id":"projects","className":"bg-muted/50 px-6 py-24","children":["$","div",null,{"className":"mx-auto max-w-5xl","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"h-px w-8 bg-primary"}],["$","span",null,{"className":"font-mono text-sm tracking-widest text-primary uppercase","children":"Projects"}]]}],["$","h2",null,{"className":"mt-2 text-3xl font-bold tracking-tight text-foreground sm:text-4xl","children":"Selected work"}],["$","p",null,{"className":"mt-4 max-w-xl text-base text-muted-foreground","children":"Open-source projects and research implementations spanning perception, calibration, and deep learning for robotics."}],["$","div",null,{"className":"mt-12 grid gap-6 md:grid-cols-2 lg:grid-cols-3","children":[["$","article","Vision-Conditioned Trajectory Generation with Flow Matching",{"className":"group flex flex-col rounded-xl border border-border bg-card p-6 transition-all hover:border-primary/30 hover:shadow-sm","children":[["$","div",null,{"className":"mb-3 flex items-center justify-between","children":[["$","h3",null,{"className":"text-base font-semibold text-card-foreground","children":"Vision-Conditioned Trajectory Generation with Flow Matching"}],["$","a",null,{"href":"https://github.com/SarveshAngadi09/TrajectoryGenerationFlowmatching","target":"_blank","rel":"noopener noreferrer","className":"text-muted-foreground transition-colors hover:text-primary","aria-label":"View Vision-Conditioned Trajectory Generation with Flow Matching on GitHub","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github h-4 w-4","aria-hidden":"true","children":[["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}],["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}],"$undefined"]}]}]]}],["$","p",null,{"className":"flex-1 text-sm leading-relaxed text-muted-foreground","children":"Implementing conditional and mean flow matching models for vision-conditioned robotic trajectory generation on the PushT dataset."}],["$","div",null,{"className":"mt-4 flex flex-wrap gap-2","children":[["$","span","Pytorch",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"Pytorch"}],["$","span","Path Planning",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"Path Planning"}],["$","span","GenAI",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"GenAI"}],["$","span","Robotics",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"Robotics"}]]}]]}],["$","article","Adversarial Attacks in Neural Networks",{"className":"group flex flex-col rounded-xl border border-border bg-card p-6 transition-all hover:border-primary/30 hover:shadow-sm","children":[["$","div",null,{"className":"mb-3 flex items-center justify-between","children":[["$","h3",null,{"className":"text-base font-semibold text-card-foreground","children":"Adversarial Attacks in Neural Networks"}],["$","a",null,{"href":"https://github.com/SarveshAngadi09/DLadversarialattack","target":"_blank","rel":"noopener noreferrer","className":"text-muted-foreground transition-colors hover:text-primary","aria-label":"View Adversarial Attacks in Neural Networks on GitHub","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github h-4 w-4","aria-hidden":"true","children":["$L1b","$L1c","$undefined"]}]}]]}],"$L1d","$L1e"]}],"$L1f"]}],"$L20"]}]}]
a:["$","section",null,{"id":"teaching","className":"px-6 py-24","children":["$","div",null,{"className":"mx-auto max-w-5xl","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"h-px w-8 bg-primary"}],["$","span",null,{"className":"font-mono text-sm tracking-widest text-primary uppercase","children":"Teaching & Talks"}]]}],["$","h2",null,{"className":"mt-2 text-3xl font-bold tracking-tight text-foreground sm:text-4xl","children":"Workshops & presentations"}],["$","p",null,{"className":"mt-4 max-w-xl text-base text-muted-foreground","children":"Sharing knowledge through workshops, talks, and seminars on computer vision, robotics, and deep learning."}],["$","div",null,{"className":"mt-12 space-y-4","children":[["$","div","0",{"className":"group rounded-xl border border-border bg-card p-6 transition-colors hover:border-primary/30","children":["$","div",null,{"className":"flex flex-col gap-4 sm:flex-row sm:items-start sm:justify-between","children":["$","div",null,{"className":"flex-1","children":[["$","div",null,{"className":"mb-2 flex items-center gap-3","children":[["$","span",null,{"className":"inline-flex items-center gap-1.5 rounded-md bg-primary/10 px-2.5 py-0.5 text-xs font-medium text-primary","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-presentation h-3 w-3","aria-hidden":"true","children":[["$","path","91anmk",{"d":"M2 3h20"}],["$","path","2k9sn8",{"d":"M21 3v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V3"}],["$","path","bip4we",{"d":"m7 21 5-5 5 5"}],"$undefined"]}],"Teaching"]}],["$","span",null,{"className":"inline-flex items-center gap-1.5 text-xs text-muted-foreground","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-calendar h-3 w-3","aria-hidden":"true","children":[["$","path","1cmpym",{"d":"M8 2v4"}],["$","path","4m81vk",{"d":"M16 2v4"}],["$","rect","1hopcy",{"width":"18","height":"18","x":"3","y":"4","rx":"2"}],["$","path","8toen8",{"d":"M3 10h18"}],"$undefined"]}],"Feb 2026"]}]]}],["$","h3",null,{"className":"text-base font-semibold text-card-foreground","children":"Vision-Driven Robotics: Theory to Practice"}],["$","p",null,{"className":"mt-1 text-sm text-muted-foreground","children":"Discord"}],["$","p",null,{"className":"mt-2 text-sm leading-relaxed text-muted-foreground/80","children":"I conduct independent sessions on Computer Vision and Robotics fundamentals for students and early learners. The sessions focus on building strong intuition behind core concepts such as camera calibration, multi-view geometry, deep learning for perception, and generative models for robotics. My goal is to bridge theory and implementation — breaking down complex ideas into clear mathematical reasoning and practical code examples."}],["$","a",null,{"href":"https://github.com/SarveshAngadi09/Robotics-CV","target":"_blank","rel":"noopener noreferrer","className":"mt-4 inline-flex items-center gap-2 text-sm font-medium text-primary transition-colors hover:underline","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github h-4 w-4","aria-hidden":"true","children":[["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}],["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}],"$undefined"]}],"View Materials on GitHub"]}]]}]}]}]]}]]}]}]
b:["$","section",null,{"id":"blog","className":"bg-muted/50 px-6 py-24","children":["$","div",null,{"className":"mx-auto max-w-5xl","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"h-px w-8 bg-primary"}],["$","span",null,{"className":"font-mono text-sm tracking-widest text-primary uppercase","children":"Blog"}]]}],["$","h2",null,{"className":"mt-2 text-3xl font-bold tracking-tight text-foreground sm:text-4xl","children":"Recent writing"}],["$","p",null,{"className":"mt-4 max-w-xl text-base text-muted-foreground","children":"Technical notes and tutorials on topics I find interesting."}],["$","div",null,{"className":"mt-12 grid gap-6 md:grid-cols-3","children":[["$","a","Normalizing Flows: Density Estimation via Change of Variables",{"href":"https://github.com/SarveshAngadi09/Robotics-CV/tree/main/NormalizingFlows","target":"_blank","rel":"noopener noreferrer","className":"group flex flex-col rounded-xl border border-border bg-card p-6 transition-all hover:border-primary/30 hover:shadow-sm","children":[["$","div",null,{"className":"mb-3 flex items-center gap-3","children":[["$","span",null,{"className":"rounded-md bg-primary/10 px-2.5 py-0.5 text-xs font-medium text-primary","children":"Generative AI"}],["$","span",null,{"className":"text-xs text-muted-foreground","children":"Feb 2026"}]]}],["$","h3",null,{"className":"text-base font-semibold leading-snug text-card-foreground","children":"Normalizing Flows: Density Estimation via Change of Variables"}],["$","p",null,{"className":"mt-2 flex-1 text-sm leading-relaxed text-muted-foreground","children":"An intuitive and mathematical introduction to normalizing flows, explaining density estimation through invertible transformations and the change-of-variables formula."}],["$","div",null,{"className":"mt-4 flex items-center justify-between","children":[["$","span",null,{"className":"font-mono text-xs text-muted-foreground","children":"8 min read"}],["$","span",null,{"className":"inline-flex items-center gap-1 text-sm font-medium text-primary opacity-0 transition-opacity group-hover:opacity-100","children":["Read",["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-right h-3.5 w-3.5","aria-hidden":"true","children":[["$","path","1ays0h",{"d":"M5 12h14"}],["$","path","xquz4c",{"d":"m12 5 7 7-7 7"}],"$undefined"]}]]}]]}]]}],["$","a","Basics of Camera Calibration",{"href":"https://github.com/SarveshAngadi09/Robotics-CV/tree/main/Calibration","target":"_blank","rel":"noopener noreferrer","className":"group flex flex-col rounded-xl border border-border bg-card p-6 transition-all hover:border-primary/30 hover:shadow-sm","children":[["$","div",null,{"className":"mb-3 flex items-center gap-3","children":[["$","span",null,{"className":"rounded-md bg-primary/10 px-2.5 py-0.5 text-xs font-medium text-primary","children":"Calibration"}],["$","span",null,{"className":"text-xs text-muted-foreground","children":"Feb 2026"}]]}],["$","h3",null,{"className":"text-base font-semibold leading-snug text-card-foreground","children":"Basics of Camera Calibration"}],["$","p",null,{"className":"mt-2 flex-1 text-sm leading-relaxed text-muted-foreground","children":"An intuitive and mathematical introduction to intrinsic camera calibration, covering focal length, principal point estimation, and lens distortion modeling for accurate vision-based perception."}],["$","div",null,{"className":"mt-4 flex items-center justify-between","children":[["$","span",null,{"className":"font-mono text-xs text-muted-foreground","children":"6 min read"}],["$","span",null,{"className":"inline-flex items-center gap-1 text-sm font-medium text-primary opacity-0 transition-opacity group-hover:opacity-100","children":["Read",["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-right h-3.5 w-3.5","aria-hidden":"true","children":["$L21","$L22","$undefined"]}]]}]]}]]}]]}]]}]}]
c:["$","section",null,{"id":"contact","className":"px-6 py-24","children":["$","div",null,{"className":"mx-auto max-w-5xl","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"h-px w-8 bg-primary"}],["$","span",null,{"className":"font-mono text-sm tracking-widest text-primary uppercase","children":"Contact"}]]}],["$","h2",null,{"className":"mt-2 text-3xl font-bold tracking-tight text-foreground sm:text-4xl","children":"Let's connect"}],["$","p",null,{"className":"mt-4 max-w-xl text-base text-muted-foreground","children":"Interested in hiring me, collaboration, research opportunities, or just want to say hello? I'd love to hear from you."}],["$","div",null,{"className":"mt-12 flex flex-col gap-8 sm:flex-row sm:items-start sm:gap-16","children":[["$","div",null,{"children":[["$","h3",null,{"className":"text-sm font-semibold uppercase tracking-wider text-foreground","children":"Email"}],["$","a",null,{"href":"mailto:sarvesh.angadi1997@gmail.com","className":"mt-2 inline-flex items-center gap-2 text-base text-primary transition-colors hover:text-primary/80","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-mail h-4 w-4","aria-hidden":"true","children":[["$","path","132q7q",{"d":"m22 7-8.991 5.727a2 2 0 0 1-2.009 0L2 7"}],["$","rect","izxlao",{"x":"2","y":"4","width":"20","height":"16","rx":"2"}],"$undefined"]}],"sarvesh.angadi1997@gmail.com"]}]]}],["$","div",null,{"children":[["$","h3",null,{"className":"text-sm font-semibold uppercase tracking-wider text-foreground","children":"Social"}],["$","ul",null,{"className":"mt-2 flex items-center gap-4","children":[["$","li","GitHub",{"children":["$","a",null,{"href":"https://github.com/SarveshAngadi09","target":"_blank","rel":"noopener noreferrer","className":"inline-flex h-10 w-10 items-center justify-center rounded-lg border border-border text-muted-foreground transition-colors hover:border-primary/30 hover:text-primary","aria-label":"GitHub","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github h-4 w-4","aria-hidden":"true","children":[["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}],["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}],"$undefined"]}]}]}],["$","li","LinkedIn",{"children":["$","a",null,{"href":"https://www.linkedin.com/in/sarvesh-angadi-067375154/","target":"_blank","rel":"noopener noreferrer","className":"inline-flex h-10 w-10 items-center justify-center rounded-lg border border-border text-muted-foreground transition-colors hover:border-primary/30 hover:text-primary","aria-label":"LinkedIn","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-linkedin h-4 w-4","aria-hidden":"true","children":[["$","path","c2jq9f",{"d":"M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"}],["$","rect","mk3on5",{"width":"4","height":"12","x":"2","y":"9"}],["$","circle","bt5ra8",{"cx":"4","cy":"4","r":"2"}],"$undefined"]}]}]}]]}]]}]]}]]}]}]
d:["$","footer",null,{"className":"border-t border-border px-6 py-8","children":["$","div",null,{"className":"mx-auto flex max-w-5xl flex-col items-center justify-between gap-4 sm:flex-row","children":[["$","p",null,{"className":"text-sm text-muted-foreground","children":"© 2026 Sarvesh Angadi. All rights reserved."}],["$","p",null,{"className":"font-mono text-xs text-muted-foreground/60","children":"Latent space"}]]}]}]
e:["$","script","script-0",{"src":"./_next/static/chunks/f813fde7a26bc4e5.js","async":true,"nonce":"$undefined"}]
f:["$","$L23",null,{"children":["$","$24",null,{"name":"Next.MetadataOutlet","children":"$@25"}]}]
10:["$","$1","h",{"children":[null,["$","$L26",null,{"children":"$L27"}],["$","div",null,{"hidden":true,"children":["$","$L28",null,{"children":["$","$24",null,{"name":"Next.Metadata","children":"$L29"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
12:["$","path","1gqd8o",{"d":"M17.598 6.5A3 3 0 1 0 12 5a3 3 0 1 0-5.598 1.5"}]
13:["$","path","iwvgf7",{"d":"M17.997 5.125a4 4 0 0 1 2.526 5.77"}]
14:["$","path","efp6ie",{"d":"M18 18a4 4 0 0 0 2-7.464"}]
15:["$","path","1gq6am",{"d":"M19.967 17.483A4 4 0 1 1 12 18a4 4 0 1 1-7.967-.517"}]
16:["$","path","k1g0md",{"d":"M6 18a4 4 0 0 1-2-7.464"}]
17:["$","path","q97ue3",{"d":"M6.003 5.125a4 4 0 0 0-2.526 5.77"}]
18:["$","h3",null,{"className":"text-base font-semibold text-card-foreground","children":"Deep Learning"}]
19:["$","p",null,{"className":"mt-2 text-sm leading-relaxed text-muted-foreground","children":"Designing and training CNNs and transformers for object detection, segmentation, and visual place recognition."}]
1a:["$","div","GenAI-Powered 3D Perception",{"className":"group rounded-xl border border-border bg-card p-6 transition-colors hover:border-primary/30","children":[["$","div",null,{"className":"mb-4 inline-flex h-10 w-10 items-center justify-center rounded-lg bg-primary/10 text-primary","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-box h-5 w-5","aria-hidden":"true","children":[["$","path","hh9hay",{"d":"M21 8a2 2 0 0 0-1-1.73l-7-4a2 2 0 0 0-2 0l-7 4A2 2 0 0 0 3 8v8a2 2 0 0 0 1 1.73l7 4a2 2 0 0 0 2 0l7-4A2 2 0 0 0 21 16Z"}],["$","path","g66t2b",{"d":"m3.3 7 8.7 5 8.7-5"}],["$","path","d0xqtd",{"d":"M12 22V12"}],"$undefined"]}]}],["$","h3",null,{"className":"text-base font-semibold text-card-foreground","children":"GenAI-Powered 3D Perception"}],["$","p",null,{"className":"mt-2 text-sm leading-relaxed text-muted-foreground","children":"Applying generative models and Gaussian Splatting to enable scalable 3D scene reconstruction, immersive mapping, and real-time spatial intelligence for robotics and AR/VR applications."}]]}]
1b:["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}]
1c:["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}]
1d:["$","p",null,{"className":"flex-1 text-sm leading-relaxed text-muted-foreground","children":"Implementation and analysis of adversarial attack strategies and defense mechanisms to study robustness in deep neural networks."}]
1e:["$","div",null,{"className":"mt-4 flex flex-wrap gap-2","children":[["$","span","Python",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"Python"}],["$","span","Pytorch",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"Pytorch"}],["$","span","OpenCV",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"OpenCV"}],["$","span","Neural Networks",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"Neural Networks"}]]}]
1f:["$","article","Mapping facial features and hand gestures",{"className":"group flex flex-col rounded-xl border border-border bg-card p-6 transition-all hover:border-primary/30 hover:shadow-sm","children":[["$","div",null,{"className":"mb-3 flex items-center justify-between","children":[["$","h3",null,{"className":"text-base font-semibold text-card-foreground","children":"Mapping facial features and hand gestures"}],["$","a",null,{"href":"https://github.com/SarveshAngadi09/Face-Hand-Interaction-Computer-vision","target":"_blank","rel":"noopener noreferrer","className":"text-muted-foreground transition-colors hover:text-primary","aria-label":"View Mapping facial features and hand gestures on GitHub","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github h-4 w-4","aria-hidden":"true","children":[["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}],["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}],"$undefined"]}]}]]}],["$","p",null,{"className":"flex-1 text-sm leading-relaxed text-muted-foreground","children":"Implementation of face and palm detection using mediapipe and openCV"}],["$","div",null,{"className":"mt-4 flex flex-wrap gap-2","children":[["$","span","Python",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"Python"}],["$","span","mediapipe",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"mediapipe"}],["$","span","OpenCV",{"className":"rounded-md bg-primary/8 px-2 py-0.5 font-mono text-xs text-primary","children":"OpenCV"}]]}]]}]
20:["$","div",null,{"className":"mt-10 text-center","children":["$","a",null,{"href":"https://github.com/SarveshAngadi09","target":"_blank","rel":"noopener noreferrer","className":"inline-flex items-center gap-2 text-sm font-medium text-primary transition-colors hover:text-primary/80","children":["View all projects on GitHub",["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-external-link h-3.5 w-3.5","aria-hidden":"true","children":[["$","path","1q9fwt",{"d":"M15 3h6v6"}],["$","path","gplh6r",{"d":"M10 14 21 3"}],["$","path","a6xqqp",{"d":"M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"}],"$undefined"]}]]}]}]
21:["$","path","1ays0h",{"d":"M5 12h14"}]
22:["$","path","xquz4c",{"d":"m12 5 7 7-7 7"}]
27:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
25:null
29:[["$","title","0",{"children":"Portfolio | Robotics & Computer Vision Engineer"}],["$","meta","1",{"name":"description","content":"Personal portfolio of a Robotics & Computer Vision engineer specializing in perception systems, deep learning, and camera calibration."}]]
